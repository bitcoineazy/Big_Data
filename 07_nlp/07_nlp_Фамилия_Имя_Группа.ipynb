{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* Макрушин С.В. Лекция \"Введение в обработку текста на естественном языке\"\n",
    "* https://www.nltk.org/api/nltk.metrics.distance.html\n",
    "* https://pymorphy2.readthedocs.io/en/stable/user/guide.html\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=1c0a9370aeca7cb470e562927d94b687b23566353b6f94e34bb73eb42eb9f59d\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/7c/d7/8d/2156234738063e3d4a39ba77dc677046100e62766b53807189\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. При помощи расстояния Левенштейна иправьте опечатку в слове \"велечайшим\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки второго задания на слова. Проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Разбейте текст из формулировки второго задания на слова. Проведите стемминг и лемматизацию слов.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 2 в векторы при помощи `CountVectorizer`. Выведите на экран словарь обученного токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Разбейте текст из формулировки второго задания на слова. Проведите стемминг и лемматизацию слов.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Загрузите данные из файла `ru_recipes_sample.csv` в виде `pd.DataFrame` `recipes` Используя регулярные выражения, удалите из описаний (столбец `description`) все символы, кроме русских букв, цифр и пробелов. Приведите все слова в описании к нижнему регистру. Сохраните полученный результат в столбец `description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       этот коктейль готовлю из замороженной клубники...\n",
       "1                                         быстро и вкусно\n",
       "2                сытный овощной салатик пальчики оближете\n",
       "3       картофельное пюре и куриные котлеты  вкусная к...\n",
       "4       вишневая наливка имеет яркий вишневый вкус кот...\n",
       "                              ...                        \n",
       "3462    для тех кто любит чечевицу вам сюда очень вкус...\n",
       "3463    баклажановые фантазии продолжаются предлагаю в...\n",
       "3464    мое любимое блюдо лазанья но кушать только фар...\n",
       "3465    прошлым летом варила варенье из одуванчиков по...\n",
       "3466     и три корочки хлеба  сделал заказ буратино в ...\n",
       "Name: description, Length: 3467, dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "recipes = pd.read_csv(\"07_nlp_data/ru_recipes_sample.csv\")\n",
    "\n",
    "#recipes[\"description\"] = recipes[\"description\"].apply(lambda x: x if re.search(r\"\\w\\w\", x) else None)\n",
    "recipes[\"description\"] = recipes[\"description\"].replace(to_replace=r\"[^А-Яа-яЁё0-9\\s]\", value=\"\", regex=True)\n",
    "recipes[\"description\"] = recipes[\"description\"].apply(lambda x: x.lower())\n",
    "\n",
    "recipes[\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние Левенштейна. Выведите на экран результат в следующем виде:\n",
    "\n",
    "```\n",
    "d(word1, word2) = x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['шокоманы',\n",
       " 'ночное',\n",
       " 'резать',\n",
       " 'овсяную',\n",
       " 'акцент',\n",
       " 'мнением',\n",
       " 'виртуально',\n",
       " 'бумагу',\n",
       " 'несладкое',\n",
       " 'поблагодарим']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(nltk.word_tokenize(\" \".join(recipes['description']).replace('\\r', ' ').replace('\\n', ' '))))\n",
    "\n",
    "unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(каштаны, православный) = 8\n",
      "d(милой, запекания) = 9\n",
      "d(говяжьей, заверить) = 7\n",
      "d(девушкам, решетке) = 6\n",
      "d(вермишель, утончен) = 8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_words = random.sample(unique_words, k=10)\n",
    "\n",
    "for i in range(0, len(random_words) - 1, 2):\n",
    "    print(f\"d({random_words[i]}, {random_words[i+1]}) = {nltk.edit_distance(random_words[i], random_words[i+1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Напишите функцию, которая принимает на вход 2 текстовые строки `s1` и `s2` и при помощи расстояния Левенштейна определяет, является ли строка `s2` плагиатом `s1`. Функция должна реализовывать следующую логику: для каждого слова `w1` из `s1` проверяет, есть в `s2` хотя бы одно слово `w2`, такое, что расстояние Левенштейна между `w1` и `w2` меньше 2, и считает количество таких слов в `s1` $P$. \n",
    "\n",
    "$$ P = \\#\\{w_1 \\in s_1\\ | \\exists w_2 \\in s_2 : d(w_1, w_2) < tol\\}$$\n",
    "\n",
    "$$ L = max(|s1|, |s2|) $$\n",
    "\n",
    "Здесь $|\\cdot|$ - количество слов в строке, $\\#A$ - число элементов в множестве $A$, $w \\in s$ означает, что слово $w$ содержится в тексте $s$.\n",
    "\n",
    "Если отношение $P / L$ больше 0.8, то функция должна вернуть True; иначе False.\n",
    "\n",
    "Продемонстрируйте работу вашей функции на примере описаний двух рецептов с ID 135488 и 851934 (ID рецепта - это число, стоящее в конце url рецепта). Выведите на экран описания этих рецептов и результат работы функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plagiarism(s1: str, s2: str) -> bool:\n",
    "    s1 = word_tokenize(s1)\n",
    "    s2 = word_tokenize(s2)\n",
    "    L = max(len(s1), len(s2))\n",
    "    P = 0\n",
    "    for word in s1:\n",
    "        for word_2 in s2:\n",
    "            if nltk.edit_distance(word, word_2) < 2:\n",
    "                P += 1\n",
    "    # print(P / L)\n",
    "    return (P / L) > 0.8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/135488/</td>\n",
       "      <td>Паштет сало-авокадо в хлебных орешках</td>\n",
       "      <td>{'Сало': '100 г', 'Соль': '1/3 ч. л.', 'Чеснок...</td>\n",
       "      <td>прекрасной закуской к крепким напиткам на фурш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/851934/</td>\n",
       "      <td>Паштет из сала и авокадов хлебных орешках</td>\n",
       "      <td>{'Сало': '100 г', 'Соль': '1/3 ч. л.', 'Чеснок...</td>\n",
       "      <td>замечательной закуской к напиткам на фуршетном...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  \\\n",
       "958   https://www.povarenok.ru/recipes/show/135488/   \n",
       "1473  https://www.povarenok.ru/recipes/show/851934/   \n",
       "\n",
       "                                           name  \\\n",
       "958       Паштет сало-авокадо в хлебных орешках   \n",
       "1473  Паштет из сала и авокадов хлебных орешках   \n",
       "\n",
       "                                            ingredients  \\\n",
       "958   {'Сало': '100 г', 'Соль': '1/3 ч. л.', 'Чеснок...   \n",
       "1473  {'Сало': '100 г', 'Соль': '1/3 ч. л.', 'Чеснок...   \n",
       "\n",
       "                                            description  \n",
       "958   прекрасной закуской к крепким напиткам на фурш...  \n",
       "1473  замечательной закуской к напиткам на фуршетном...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рецепты с нужным нам ID\n",
    "\n",
    "recipes[(recipes[\"url\"].str.endswith(\"851934/\")) | (recipes[\"url\"].str.endswith(\"135488/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plagiarism(recipes[recipes[\"url\"].str.endswith(\"135488/\")][\"description\"].values[0], recipes[recipes[\"url\"].str.endswith(\"851934/\")][\"description\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. На основе набора слов из задания 2 создайте `pd.DataFrame` со столбцами `word`, `stemmed_word` и `normalized_word`. В столбец `stemmed_word` поместите версию слова после проведения процедуры стемминга; в столбец `normalized_word` поместите версию слова после проведения процедуры лемматизации. Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга можно воспользоваться `SnowballStemmer` из `nltk`, для лемматизации слов - пакетом `pymorphy2`. Сравните результаты стемминга и лемматизации. Поясните на примере одной из строк получившегося фрейма (в виде текстового комментария), в чем разница между двумя этими подходами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>шокоманы</th>\n",
       "      <td>шокома</td>\n",
       "      <td>шокоман</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ночное</th>\n",
       "      <td>ночн</td>\n",
       "      <td>ночной</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>резать</th>\n",
       "      <td>реза</td>\n",
       "      <td>резать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>овсяную</th>\n",
       "      <td>овсян</td>\n",
       "      <td>овсяный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>акцент</th>\n",
       "      <td>акцент</td>\n",
       "      <td>акцент</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>выбрать</th>\n",
       "      <td>выбра</td>\n",
       "      <td>выбрать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>дичи</th>\n",
       "      <td>дич</td>\n",
       "      <td>дичь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>готовлюсь</th>\n",
       "      <td>готовл</td>\n",
       "      <td>готовиться</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>капустных</th>\n",
       "      <td>капустн</td>\n",
       "      <td>капустный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>чаю</th>\n",
       "      <td>ча</td>\n",
       "      <td>чай</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16451 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          stemmed_word normalized_word\n",
       "word                                  \n",
       "шокоманы        шокома         шокоман\n",
       "ночное            ночн          ночной\n",
       "резать            реза          резать\n",
       "овсяную          овсян         овсяный\n",
       "акцент          акцент          акцент\n",
       "...                ...             ...\n",
       "выбрать          выбра         выбрать\n",
       "дичи               дич            дичь\n",
       "готовлюсь       готовл      готовиться\n",
       "капустных      капустн       капустный\n",
       "чаю                 ча             чай\n",
       "\n",
       "[16451 rows x 2 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stem_lem_df = pd.DataFrame(columns=['word', 'stemmed_word', 'normalized_word'])\n",
    "stem_lem_df['word'] = unique_words\n",
    "stem_lem_df['stemmed_word'] = [stemmer.stem(word) for word in stem_lem_df['word']]\n",
    "stem_lem_df['normalized_word'] = [morph.parse(word)[0].normalized.word for word in stem_lem_df['word']]\n",
    "stem_lem_df = stem_lem_df.set_index('word')\n",
    "stem_lem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Добавьте в таблицу `recipes` столбец `description_no_stopwords`, в котором содержится текст описания рецепта после удаления из него стоп-слов. Посчитайте и выведите на экран долю стоп-слов среди общего количества слов. Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7395/1450924871.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  recipes['description_no_stopwords'] = recipes['description'].str.replace(pattern, '')\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words('russian')\n",
    "pattern = r'\\b(?:{})\\b'.format('|'.join(stops))\n",
    "recipes['description_no_stopwords'] = recipes['description'].str.replace(pattern, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102978, 33223, 0.322622307677368)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "word_counts_before = Counter(word_tokenize(' '.join(recipes['description'])))\n",
    "count_before, count_stops = sum(word_counts_before.values()), sum([word_counts_before[x] for x in stops])\n",
    "count_before, count_stops, count_stops / count_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69755, 69755)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_after = Counter(word_tokenize(' '.join(recipes['description_no_stopwords'])))\n",
    "word_count_before = word_counts_before.most_common(10)\n",
    "word_count_after = word_counts_after.most_common(10)\n",
    "sum(word_counts_after.values()), count_before - count_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 5054),\n",
       " ('в', 2584),\n",
       " ('с', 1934),\n",
       " ('на', 1655),\n",
       " ('очень', 1607),\n",
       " ('не', 1517),\n",
       " ('из', 1006),\n",
       " ('я', 979),\n",
       " ('рецепт', 869),\n",
       " ('а', 863)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('очень', 1607),\n",
       " ('рецепт', 869),\n",
       " ('это', 734),\n",
       " ('блюдо', 524),\n",
       " ('вкусный', 461),\n",
       " ('просто', 436),\n",
       " ('вкусно', 375),\n",
       " ('приготовить', 344),\n",
       " ('вкус', 324),\n",
       " ('салат', 313)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Выберите случайным образом 5 рецептов из набора данных, в названии которых есть слово \"оладьи\" (без учета регистра). Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`. На основе полученных векторов создайте `pd.DataFrame`, в котором названия колонок соответствуют словам из словаря объекта-векторизатора. \n",
    "\n",
    "Примечание: обратите внимание на порядок слов при создании колонок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>нежные</th>\n",
       "      <th>сочные</th>\n",
       "      <th>оладьи</th>\n",
       "      <th>очень</th>\n",
       "      <th>вкусные</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.83205</td>\n",
       "      <td>0.27735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    нежные   сочные   оладьи    очень  вкусные\n",
       "0  0.27735  0.27735  0.27735  0.83205  0.27735"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "olad = recipes[recipes['name'].str.contains('оладьи')]\n",
    "sample = olad.sample(n=5)\n",
    "sample\n",
    "\n",
    "def get_dataframe_vect(sent_words):\n",
    "    cv = TfidfVectorizer()\n",
    "    cv.fit(sent_words)\n",
    "    return pd.DataFrame([{elem: cv.transform(sent_words).toarray()[0][cv.vocabulary_[elem]] for elem in cv.vocabulary_}])\n",
    "\n",
    "get_dataframe_vect(nltk.sent_tokenize(sample['description'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Вычислите близость между каждой парой рецептов, выбранных в задании 6, используя косинусное расстояние (можно воспользоваться функциями из любого пакета: `scipy`, `scikit-learn` или реализовать функцию самому). Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов.\n",
    "\n",
    "Примечание: обратите внимание, что $d_{cosine}(x, x) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напишите функцию, которая принимает на вход `pd.DataFrame`, полученный в задании 7, и возвращает в виде кортежа названия двух различных рецептов, которые являются наиболее похожими. Прокомментируйте результат (в виде текстового комментария). Для объяснения результата сравните слова в описаниях двух этих отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(sim_df: pd.DataFrame) -> tuple:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 1]\n",
      " [2 0 1 1 0 1]]\n",
      "['document', 'first', 'is', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is second document.']\n",
    "cv = CountVectorizer().fit(corpus)\n",
    "print(cv.transform(corpus).toarray())\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "# аrrаy([[1, 1, 1, 0, 1, 1],\n",
    "#        [2, 0, 1, 1, 1, 1]], dtypе=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
